Initialisation
	Zero,Random and He initialisations
	L2 Regularization
	Dropout - inverted dropout. Divide A[L] and dA[L]/Keep_prob
	Gradient checking - 1 hidden layer, Multi layer
